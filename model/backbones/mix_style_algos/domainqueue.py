import math
import random
import torch
import torch.nn as nn

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class DomainQueue(nn.Module):

    def __init__(self, num_features, num_domains, p=0.5, alpha=0.1, eps=1e-6, mix='diff_domain', capacity=1024, syn_capacity=1024):
        """
        Args:
          p (float): probability of using mix.
          alpha (float): parameter of the Beta distribution.
          eps (float): scaling parameter to avoid numerical issues.
          mix (str): how to mix. (random / diff_domain)
        """
        super().__init__()
        self.num_features = num_features
        self.num_domains = num_domains
        self.p = p
        self.beta = torch.distributions.Beta(alpha, alpha)
        self.eps = eps
        self.alpha = alpha
        self.mix = mix
        
        self.sum = list(0 for _ in range(num_domains + 1))
        self.capacity = capacity
        self.syn_capacity = syn_capacity
        limitation = max(capacity, syn_capacity)
        self.register_buffer('mean_queue', torch.zeros(num_domains + 1, limitation, num_features))
        self.register_buffer('sig_queue', torch.ones(num_domains + 1, limitation, num_features))
        self.mean_queue.requires_grad = False
        self.sig_queue.requires_grad = False

        trunc_normal_(self.mean_queue)
        trunc_normal_(self.sig_queue)

    def forward(self, x, domain=None):
        if not self.training:
            return x

        for i in range(self.num_domains):
            if not i in domain:
                continue
            mean = x[domain==i].mean(1).detach()
            sig = (x[domain==i].var(1)+self.eps).sqrt().detach()
            length = (domain==i).sum()

            sum = self.sum[i] % self.capacity
            rest = self.capacity - sum
            if length > rest:
                self.mean_queue[i, sum:sum+rest] = mean[:rest]
                self.mean_queue[i, :length-rest] = mean[rest:]
                self.sig_queue[i, sum:sum+rest] = sig[:rest]
                self.sig_queue[i, :length-rest] = sig[rest:]
            else:
                self.mean_queue[i, sum:sum+length] = mean
                self.sig_queue[i, sum:sum+length] = sig
            self.sum[i] = self.sum[i] + int(length)

        if not random.random() > self.p:
            return x
        
        B = x.size(0)
        mu = x.mean(dim=1, keepdim=True) ### origin dim=1
        var = x.var(dim=1, keepdim=True)
        sig = (var + self.eps).sqrt()
        mu, sig = mu.detach(), sig.detach()
        x_normed = (x-mu) / sig

        lmda = self.beta.sample((B, 1, 1))
        lmda = lmda.to(x.device)

        if self.mix == 'random':
            d_ind1 = torch.randint_like(domain, 0, self.num_domains+1)
        #### make sure that inds are all different from domain
        elif self.mix == 'diff_domain':
            d_ind1 = torch.zeros_like(domain)
            for i in range(B):
                # lst = list(range(0, self.num_domains)) # no sync
                lst = list(range(0, self.num_domains+1)) # sync
                lst.remove(domain[i])
                d_ind1[i] = random.choice(lst)
        else:
            assert False, "not implemented mix way: {}".format(self.mix)
        ranges = torch.zeros_like(d_ind1)
        for i, d in enumerate(d_ind1):
            if d < self.num_domains:
                ranges[i] = self.sum[d] % self.capacity
                # print(d)
            else:
                ranges[i] = self.sum[d] % self.syn_capacity
        f_ind1 = torch.tensor([random.randint(0, ranges[i]) for i in range(B)])
        mu1 = self.mean_queue[d_ind1, f_ind1].unsqueeze(1)
        sig1 = self.sig_queue[d_ind1, f_ind1].unsqueeze(1)

        #### novel style enqueue
        sum = self.sum[-1] % self.syn_capacity
        rest = self.syn_capacity - sum
        if B > rest:
            self.mean_queue[-1, sum:sum+rest] = mu_mix.squeeze()[:rest]
            self.mean_queue[-1, :B-rest] = mu_mix.squeeze()[rest:]
            self.sig_queue[-1, sum:sum+rest] = sig_mix.squeeze()[:rest]
            self.sig_queue[-1, :B-rest] = sig_mix.squeeze()[rest:]
        else:
            self.mean_queue[-1, sum:sum+B] = mu_mix.squeeze()
            self.sig_queue[-1, sum:sum+B] = sig_mix.squeeze()
        self.sum[-1] = self.sum[-1] + int(B)

        return x_normed*sig_mix + mu_mix


class DomainQueue_2d(nn.Module):
    def __init__(self, num_features, num_domains, p=0.5, alpha=0.1, eps=1e-6, mix='diff_domain', capacity=1024, syn_capacity=1024):
        super().__init__()
        self.num_features = num_features
        self.num_domains = num_domains
        self.p = p
        self.beta = torch.distributions.Beta(alpha, alpha)
        self.eps = eps
        self.alpha = alpha
        self.mix = mix
        
        self.sum = list(0 for _ in range(num_domains + 1))
        self.capacity = capacity
        self.syn_capacity = syn_capacity
        limitation = max(capacity, syn_capacity)
        self.register_buffer('mean_queue', torch.zeros(num_domains + 1, limitation, num_features))
        self.register_buffer('sig_queue', torch.ones(num_domains + 1, limitation, num_features))
        self.mean_queue.requires_grad = False
        self.sig_queue.requires_grad = False

        trunc_normal_(self.mean_queue)
        trunc_normal_(self.sig_queue)

    def forward(self, x, domain=None):
        if not self.training:
            return x

        for i in range(self.num_domains):
            if not i in domain:
                continue
            mean = x[domain==i].mean([2,3]).detach()
            sig = (x[domain==i].var([2,3])+self.eps).sqrt().detach()
            length = (domain==i).sum()

            sum = self.sum[i] % self.capacity
            rest = self.capacity - sum
            if length > rest:
                self.mean_queue[i, sum:] = mean[:rest]
                self.mean_queue[i, :length-rest] = mean[rest:]
                self.sig_queue[i, sum:] = sig[:rest]
                self.sig_queue[i, :length-rest] = sig[rest:]
            else:
                self.mean_queue[i, sum:sum+length] = mean
                self.sig_queue[i, sum:sum+length] = sig
            self.sum[i] = self.sum[i] + int(length)

        if not random.random() > self.p:
            return x
        
        B = x.size(0)
        mu = x.mean(dim=[2,3], keepdim=True)
        var = x.var(dim=[2,3], keepdim=True)
        sig = (var + self.eps).sqrt()
        mu, sig = mu.detach(), sig.detach()
        x_normed = (x-mu) / sig

        lmda = self.beta.sample((B, 1, 1, 1))
        lmda = lmda.to(x.device)

        if self.mix == 'random':
            d_ind1 = torch.randint_like(domain, 0, self.num_domains+1)
        #### make sure that inds are all different from domain
        elif self.mix == 'diff_domain':
            d_ind1 = torch.zeros_like(domain)
            for i in range(B):
                # lst = list(range(0, self.num_domains)) # no sync
                lst = list(range(0, self.num_domains+1)) # sync
                lst.remove(domain[i])
                d_ind1[i] = random.choice(lst)
        else:
            assert False, "not implemented mix way: {}".format(self.mix)
        ranges = torch.zeros_like(d_ind1)
        for i, d in enumerate(d_ind1):
            if d < self.num_domains:
                ranges[i] = self.sum[d] % self.capacity
                # print(d)
            else:
                ranges[i] = self.sum[d] % self.syn_capacity
        f_ind1 = torch.tensor([random.randint(0, ranges[i]) for i in range(B)])
        mu1 = self.mean_queue[d_ind1, f_ind1][:, :, None, None]
        sig1 = self.sig_queue[d_ind1, f_ind1][:, :, None, None]
    
        #### mixstyle like
        mu_mix = mu*lmda + mu1 * (1-lmda)
        sig_mix = sig*lmda + sig1 * (1-lmda)

        #### novel style enqueue
        sum = self.sum[-1] % self.syn_capacity
        rest = self.syn_capacity - sum
        if B > rest:
            self.mean_queue[-1, sum:] = mu_mix.squeeze()[:rest]
            self.mean_queue[-1, :B-rest] = mu_mix.squeeze()[rest:]
            self.sig_queue[-1, sum:] = sig_mix.squeeze()[:rest]
            self.sig_queue[-1, :B-rest] = sig_mix.squeeze()[rest:]
        else:
            self.mean_queue[-1, sum:sum+B] = mu_mix.squeeze()
            self.sig_queue[-1, sum:sum+B] = sig_mix.squeeze()
        self.sum[-1] = self.sum[-1] + int(B)

        return x_normed*sig_mix + mu_mix